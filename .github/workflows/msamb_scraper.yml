# .github/workflows/apmc_scraper.yml
# Multi-State APMC Market Price Scraper
# Supports running all states or specific states

name: Multi-State APMC Scraper

on:
  # Schedule - run daily for all states
  schedule:
    - cron: '30 20 * * *'  # 2:00 AM IST daily
  
  # Manual trigger with options
  workflow_dispatch:
    inputs:
      state_filter:
        description: 'State code to scrape (leave empty for all)'
        required: false
        type: choice
        options:
          - ''  # All states
          - 'MH'  # Maharashtra
          - 'KA'  # Karnataka
          - 'UP'  # Uttar Pradesh
          - 'PB'  # Punjab
      organization:
        description: 'Specific organization (overrides state filter)'
        required: false
        type: choice
        options:
          - ''
          - 'MSAMB'
          - 'KARNATAKA_APMC'
          - 'UP_APMC'
          - 'PUNJAB_APMC'

env:
  PYTHON_VERSION: '3.11'
  TIMEZONE: 'Asia/Kolkata'

jobs:
  scrape-apmc-data:
    name: Scrape APMC Market Prices
    runs-on: ubuntu-latest
    timeout-minutes: 180  # 3 hours for multi-state

    steps:
      - name: üì• Checkout Repository
        uses: actions/checkout@v4

      - name: üêç Set up Python
        uses: actions/setup-python@v5
        with:
          python-version: ${{ env.PYTHON_VERSION }}
          cache: 'pip'

      - name: üì¶ Install Dependencies
        run: |
          python -m pip install --upgrade pip
          pip install -r requirements.txt
          echo "‚úÖ Dependencies installed"

      - name: üîç Verify Scraper Files
        run: |
          echo "Checking required files..."
          
          if [ ! -f "apmc_scraper.py" ]; then
            echo "‚ùå apmc_scraper.py not found"
            exit 1
          fi
          
          # Check for state-specific commodity files
          echo ""
          echo "Available commodity files:"
          ls -lh data/*.html 2>/dev/null || echo "‚ö†Ô∏è  No commodity HTML files found"
          
          echo ""
          echo "‚úÖ Scraper files verified"
      - name: üîó Test Supabase Connection
        env:
          SUPABASE_URL: ${{ secrets.SUPABASE_URL }}
          SUPABASE_SERVICE_KEY: ${{ secrets.SUPABASE_SERVICE_KEY }}
        run: |
          python -c '
          import os
          from supabase import create_client
      
          url = os.getenv("SUPABASE_URL")
          key = os.getenv("SUPABASE_SERVICE_KEY")
      
          if not url or not key:
              print("‚ùå Supabase credentials not set")
              exit(1)
      
          print(f"üîó Testing connection to: {url}")
      
          try:
              supabase = create_client(url, key)
              response = supabase.table("agri_market_sources").select("*").eq("active", True).execute()
              active_count = len(response.data) if response.data else 0
      
              print("")
              print("‚úÖ Supabase connection successful!")
              print(f"   ‚úì Active sources: {active_count}")
      
              if active_count == 0:
                  print("")
                  print("‚ö†Ô∏è  No active sources found")
                  print("   Please insert source configurations in agri_market_sources table")
                  exit(1)
      
              # Show active sources
              for source in response.data:
                  print("   ‚Ä¢ {} ({})".format(
                      source.get("organization"),
                      source.get("state_code")
                  ))
      
          except Exception as e:
              print("")
              print(f"‚ùå Connection failed: {str(e)}")
              exit(1)
          '
      - name: üöÄ Run APMC Scraper
        env:
          SUPABASE_URL: ${{ secrets.SUPABASE_URL }}
          SUPABASE_SERVICE_KEY: ${{ secrets.SUPABASE_SERVICE_KEY }}
          TZ: ${{ env.TIMEZONE }}
        run: |
          echo "========================================"
          echo "Multi-State APMC Scraper"
          echo "========================================"
          echo "Date: $(date '+%Y-%m-%d %H:%M:%S %Z')"
          echo "Run ID: ${{ github.run_number }}"
          echo ""
          
          # Build command with filters
          CMD="python apmc_scraper.py"
          
          if [ -n "${{ github.event.inputs.organization }}" ]; then
            echo "Mode: Single organization"
            echo "Organization: ${{ github.event.inputs.organization }}"
            CMD="$CMD --org ${{ github.event.inputs.organization }}"
          elif [ -n "${{ github.event.inputs.state_filter }}" ]; then
            echo "Mode: State filtered"
            echo "State: ${{ github.event.inputs.state_filter }}"
            CMD="$CMD --state ${{ github.event.inputs.state_filter }}"
          else
            echo "Mode: All active sources"
          fi
          
          echo ""
          echo "Executing: $CMD"
          echo "========================================"
          echo ""
          
          $CMD
          
          EXIT_CODE=$?
          echo ""
          echo "========================================"
          if [ $EXIT_CODE -eq 0 ]; then
            echo "‚úÖ Scraper completed successfully"
          else
            echo "‚ùå Scraper failed with exit code: $EXIT_CODE"
          fi
          echo "========================================"
          
          exit $EXIT_CODE

      - name: üìã Upload Logs
        if: always()
        uses: actions/upload-artifact@v4
        with:
          name: apmc-scraper-logs-${{ github.run_number }}
          path: |
            apmc_scraper.log
            *.log
          retention-days: 30

      - name: üìä Generate Report
        if: success()
        env:
          SUPABASE_URL: ${{ secrets.SUPABASE_URL }}
          SUPABASE_SERVICE_KEY: ${{ secrets.SUPABASE_SERVICE_KEY }}
        run: |
          python -c '
          import os
          from datetime import datetime, timedelta
          from supabase import create_client
          
          supabase = create_client(
              os.getenv("SUPABASE_URL"),
              os.getenv("SUPABASE_SERVICE_KEY")
          )
          
          # Get today data by state
          today = datetime.now().date().isoformat()
          response = supabase.table("market_prices") \
              .select("state, crop_name", count="exact") \
              .eq("price_date", today) \
              .execute()
          
          # Group by state
          state_counts = {}
          if response.data:
              for record in response.data:
                  state = record.get("state", "UNKNOWN")
                  state_counts[state] = state_counts.get(state, 0) + 1
          
          print("")
          print("=" * 60)
          print("üìä SCRAPING SUMMARY REPORT")
          print("=" * 60)
          print("Date:", datetime.now().strftime("%Y-%m-%d %H:%M:%S"))
          print("Total Records Today:", sum(state_counts.values()))
          print("")
          print("Records by State:")
          for state, count in sorted(state_counts.items()):
              print(f"  ‚Ä¢ {state}: {count:,} records")
          print("=" * 60)
          '

      - name: üö® Create Issue on Failure
        if: failure()
        uses: actions/github-script@v7
        with:
          script: |
            const runUrl = `${context.serverUrl}/${context.repo.owner}/${context.repo.repo}/actions/runs/${context.runId}`;
            const date = new Date().toISOString().split('T')[0];
            
            await github.rest.issues.create({
              owner: context.repo.owner,
              repo: context.repo.repo,
              title: `üö® APMC Scraper Failed - ${date}`,
              body: `## Multi-State APMC Scraper Failure
              
              **Time:** ${new Date().toLocaleString('en-IN', { timeZone: 'Asia/Kolkata' })}
              **Run:** [#${context.runNumber}](${runUrl})
              **Mode:** ${{ github.event.inputs.organization || github.event.inputs.state_filter || 'All States' }}
              
              ---
              
              ### üîç Investigation Steps
              
              1. [View Logs](${runUrl}#artifacts)
              2. Check Supabase connection
              3. Verify state website availability
              4. Review error patterns
              
              ### üìä Quick Checks
              
              - [ ] Supabase credentials valid
              - [ ] Database tables exist
              - [ ] Source configurations active
              - [ ] Commodity HTML files present
              - [ ] Target websites accessible
              
              ---
              
              *ü§ñ Auto-generated by GitHub Actions*`,
              labels: ['scraper-error', 'automated', 'multi-state']
            });

# Optional: Separate job for notifications
  notify:
    name: Send Notifications
    needs: scrape-apmc-data
    if: always()
    runs-on: ubuntu-latest
    
    steps:
      - name: üìß Notification Summary
        run: |
          if [ "${{ needs.scrape-apmc-data.result }}" == "success" ]; then
            echo "‚úÖ Multi-state scraping completed successfully"
          else
            echo "‚ùå Multi-state scraping failed"
          fi
